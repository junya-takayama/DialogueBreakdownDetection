{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "import keras\n",
    "from keras.layers import Merge, LSTM, Dense,GRU,SimpleRNN\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import LinearSVC\n",
    "from pprint import pprint\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pad_size = 15\n",
    "class Classifier:\n",
    "    def __init__(self,n_class=3,batch_size=100,pad_size=20):\n",
    "        encoder_a = Sequential()\n",
    "        encoder_a.add(SimpleRNN(output_dim=50, batch_input_shape=(None, pad_size, 100), return_sequences=False,dropout_U=0.4))\n",
    "        #encoder_a.add(GRU(100, input_shape=(timesteps, data_dim)))\n",
    "\n",
    "        encoder_b = Sequential()\n",
    "        encoder_b.add(SimpleRNN(output_dim=50, batch_input_shape=(None, pad_size, 100), return_sequences=False,dropout_U=0.4))\n",
    "        #encoder_b.add(GRU(100, input_shape=(timesteps, data_dim)))\n",
    "\n",
    "        decoder = Sequential()\n",
    "        decoder.add(Merge([encoder_a, encoder_b], mode='concat'))\n",
    "        decoder.add(Dense(n_class, activation='softmax'))\n",
    "\n",
    "        decoder.compile(loss='categorical_crossentropy',\n",
    "                        optimizer='rmsprop',\n",
    "                        metrics=['mean_squared_error'])\n",
    "        self.decoder = decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clustering(train_user,train_system,valid_user,valid_system,y_train,y_valid,n=3):\n",
    "    splitPoint = len(train_user)\n",
    "    user = np.sum(np.r_[train_user,valid_user],axis=1)\n",
    "    system = np.sum(np.r_[train_system,valid_system],axis=1)\n",
    "    vec = np.c_[system,user]\n",
    "    kmeans = KMeans(n_clusters=n)\n",
    "    kmeans.fit(vec)\n",
    "    result_train_user,result_valid_user = dataSplit(train_user,valid_user,splitPoint,kmeans,n)\n",
    "    result_train_system,result_valid_system = dataSplit(train_system,valid_system,splitPoint,kmeans,n)\n",
    "    result_train_y,result_valid_y = dataSplit(y_train,y_valid,splitPoint,kmeans,n)\n",
    "    return kmeans,result_train_user,result_train_system,result_valid_user,result_valid_system,result_train_y,result_valid_y\n",
    "\n",
    "def dataSplit(train,valid,splitPoint,kmeans,n):\n",
    "    result_train = [[] for i in range(n)]\n",
    "    result_valid = [[] for i in range(n)]\n",
    "    labels_train = kmeans.labels_[:splitPoint]\n",
    "    labels_valid = kmeans.labels_[splitPoint:]\n",
    "    for i,label in enumerate(labels_train):\n",
    "        result_train[label].append(train[i])\n",
    "    for i,label in enumerate(labels_valid):\n",
    "        result_valid[label].append(valid[i])\n",
    "    return result_train,result_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binarize(labels):\n",
    "    return np.array([[0,0,1] if label[2] > 0.5 else [0,1,0] if label[1] > 0.5 else [1,0,0] for label in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f_measure(model,values=[1]):\n",
    "    P = 0\n",
    "    C = 0\n",
    "    R = 0\n",
    "    for pred,corr in zip(model.predict([x_val_a,x_val_b]),y_val):\n",
    "        if pred.argmax() in values:\n",
    "            P += 1\n",
    "        if corr.argmax() in values:\n",
    "            C += 1\n",
    "        if pred.argmax() in values and corr.argmax() in values:\n",
    "            R += 1\n",
    "\n",
    "    recall = R/P if P>0 else 0\n",
    "    precision = R/C if C>0 else 0\n",
    "    try:\n",
    "        f = (2*recall*precision)/(recall+precision)\n",
    "    except:\n",
    "        f = 0\n",
    "    return {\"recall\":recall,\"precision\":precision,\"f_measure\":f}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('corpus.pickle',mode='rb') as f:\n",
    "    corpus = pickle.load(f)\n",
    "# generate dummy training data\n",
    "\n",
    "user,system,labels = zip(*corpus['vectorized']['train'])\n",
    "x_train_a_all = sequence.pad_sequences(user,pad_size,dtype=np.float32)\n",
    "#x_train_a = sequence.pad_sequences(system,pad_size,dtype=np.float32)\n",
    "x_train_b_all = sequence.pad_sequences(system,pad_size,dtype=np.float32)\n",
    "y_train_all = np.array(labels)\n",
    "\n",
    "# generate dummy validation data\n",
    "user,system,labels = zip(*corpus['vectorized']['valid'])\n",
    "x_val_a_all = sequence.pad_sequences(system,pad_size,dtype=np.float32)\n",
    "x_val_b_all = sequence.pad_sequences(user,pad_size,dtype=np.float32)\n",
    "y_val_all = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(np.array([len(corpus[\"tokenized\"][\"train\"][i][1]) for i in range(len(corpus[\"tokenized\"][\"train\"]))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 1\n",
    "kmeans_model,train_user_cluster,train_system_cluster,valid_user_cluster,valid_system_cluster,train_y_cluster,valid_y_cluster \\\n",
    "    = clustering(x_train_a_all,x_train_b_all,x_val_a_all,x_val_b_all,y_train_all,y_val_all,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/takayama/.local/lib/python3.5/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(batch_input_shape=(None, 15,..., return_sequences=False, units=50, recurrent_dropout=0.4)`\n",
      "  \"\"\"\n",
      "/home/takayama/.local/lib/python3.5/site-packages/ipykernel_launcher.py:9: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(batch_input_shape=(None, 15,..., return_sequences=False, units=50, recurrent_dropout=0.4)`\n",
      "  if __name__ == '__main__':\n",
      "/home/takayama/.local/lib/python3.5/site-packages/ipykernel_launcher.py:13: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  del sys.path[0]\n",
      "/home/takayama/.local/lib/python3.5/site-packages/keras/models.py:837: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 81312 samples, validate on 1100 samples\n",
      "Epoch 1/4\n",
      "81312/81312 [==============================] - 4s - loss: 1.0203 - mean_squared_error: 0.2036 - val_loss: 1.2109 - val_mean_squared_error: 0.1120\n",
      "Epoch 2/4\n",
      "81312/81312 [==============================] - 4s - loss: 0.9804 - mean_squared_error: 0.1956 - val_loss: 1.1498 - val_mean_squared_error: 0.1021\n",
      "Epoch 3/4\n",
      "81312/81312 [==============================] - 3s - loss: 0.9711 - mean_squared_error: 0.1938 - val_loss: 1.1240 - val_mean_squared_error: 0.1004\n",
      "Epoch 4/4\n",
      "81312/81312 [==============================] - 3s - loss: 0.9677 - mean_squared_error: 0.1932 - val_loss: 1.0705 - val_mean_squared_error: 0.0919\n"
     ]
    }
   ],
   "source": [
    "result = {\"numOfClusters\":n,\"kmeansModel\":kmeans_model,\"classifiers\":[],\"histories\":[]}\n",
    "for i in range(n):\n",
    "    x_train_a = np.array(train_user_cluster[i])\n",
    "    x_train_b = np.array(train_system_cluster[i])\n",
    "    y_train = np.array(train_y_cluster[i])\n",
    "    \n",
    "    x_val_a = np.array(valid_user_cluster[i])\n",
    "    x_val_b = np.array(valid_system_cluster[i])\n",
    "    y_val = np.array(valid_y_cluster[i])\n",
    "\n",
    "    decoder = Classifier(pad_size=pad_size).decoder\n",
    "    history = decoder.fit([x_train_a, x_train_b], y_train,\n",
    "                batch_size=512, nb_epoch=4,\n",
    "                validation_data=([x_val_a, x_val_b], y_val),shuffle=True)\n",
    "    result[\"classifiers\"].append({\n",
    "        \"O\":f_measure(decoder,[0]),\n",
    "        \"T\":f_measure(decoder,[1]),\n",
    "        \"X\":f_measure(decoder,[2]),\n",
    "        \"T-X\":f_measure(decoder,[1,2]),\n",
    "        \"model\":decoder,\n",
    "    })\n",
    "    result[\"histories\"].append(history)\n",
    "    x = list(range(len(results.history[\"loss\"])))\n",
    "    plt.plot(x, results.history['loss'], label='loss')\n",
    "    plt.plot(x, results.history['val_loss'], label='loss')\n",
    "    plt.plot(x, results.history['val_mean_squared_error'], label='loss')\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fbd7a7101d0>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = list(range(len(results.history[\"loss\"])))\n",
    "plt.figure()\n",
    "plt.xlabel(\"number of epoch\")\n",
    "plt.ylabel(\"value of loss / mse\")\n",
    "plt.title(\"learning curve of cluster \"+str(i))\n",
    "plt.plot(x, results.history['loss'], label='train_loss')\n",
    "plt.plot(x, results.history['val_loss'], label='val_loss')\n",
    "plt.plot(x, results.history['val_mean_squared_error'], label='val_mean_squared_error')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifiers': [{'O': {'f_measure': 0.7285886610373944,\n",
       "    'precision': 0.760705289672544,\n",
       "    'recall': 0.6990740740740741},\n",
       "   'T': {'f_measure': 0, 'precision': 0.0, 'recall': 0.0},\n",
       "   'T-X': {'f_measure': 0.16974169741697417,\n",
       "    'precision': 0.1503267973856209,\n",
       "    'recall': 0.19491525423728814},\n",
       "   'X': {'f_measure': 0.15593220338983052,\n",
       "    'precision': 0.1402439024390244,\n",
       "    'recall': 0.17557251908396945},\n",
       "   'model': <keras.models.Sequential at 0x7fbd7e3c59b0>}],\n",
       " 'histories': [<keras.callbacks.History at 0x7fbd7ada9eb8>],\n",
       " 'kmeansModel': KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "     n_clusters=1, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "     random_state=None, tol=0.0001, verbose=0),\n",
       " 'numOfClusters': 1}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"test.pickle\",\"wb\") as f:\n",
    "    pickle.dump([,\"a\"],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
