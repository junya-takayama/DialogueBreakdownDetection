{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "import keras\n",
    "import os\n",
    "from keras.layers import Merge, LSTM, Dense,GRU, SimpleRNN, core, Dropout, InputLayer\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    def __init__(self,n_class=3,batch_size=100,pad_size=20,wvdim=1024):\n",
    "        encoder_a = Sequential()\n",
    "        #encoder_a.add(GRU(output_dim=wvdim, return_sequences=True,dropout_U=0.6,batch_input_shape=(None, pad_size, wvdim)))\n",
    "        encoder_a.add(LSTM(output_dim=wvdim, return_sequences=False,dropout_U=0.5,batch_input_shape=(None, pad_size, wvdim)))\n",
    "        \n",
    "        encoder_b = Sequential()\n",
    "        #encoder_b.add(GRU(output_dim=wvdim, return_sequences=True,dropout_U=0.6,batch_input_shape=(None, pad_size, wvdim)))\n",
    "        encoder_b.add(LSTM(output_dim=wvdim, return_sequences=False,dropout_U=0.5,batch_input_shape=(None, pad_size, wvdim)))\n",
    "        \n",
    "        decoder = Sequential()\n",
    "        decoder.add(Merge([encoder_a, encoder_b], mode='concat'))\n",
    "        decoder.add(core.Dropout(0.4))\n",
    "        #decoder.add(Dense(1000, activation='relu'))\n",
    "        #decoder.add(core.Dropout(0.4))\n",
    "        decoder.add(Dense(500, activation='tanh'))\n",
    "        decoder.add(core.Dropout(0.3))\n",
    "        decoder.add(Dense(n_class, activation='softmax'))\n",
    "        adam = Adam(lr = 0.0005)\n",
    "        decoder.compile(loss= 'mse',\n",
    "                        optimizer=adam\n",
    "                       )\n",
    "        open('./models/classifier/'+namehead+'_k_'+str(n_clusters)+'.yaml','w').write(decoder.to_yaml())\n",
    "        self.decoder = decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_annotations = pd.read_pickle('./data/annotations.pickle')\n",
    "#tmp = [[vec for vec in data if vec != 'a'] for data in df_annotations.fillna('a').values]\n",
    "#labels = np.array([np.mean(vec,axis=0) for vec in tmp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_vecs = pd.read_pickle('./data/vecs.pickle')\n",
    "users = df_vecs['user'].tolist()\n",
    "systems = df_vecs['system'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_clusters = 3\n",
    "kmeans = KMeans(n_clusters=n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "annt_dist = [(annt,df_annotations[annt].mean(),df_annotations[annt].count()) for annt in df_annotations.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_dist = pd.DataFrame(annt_dist,columns=['annt_id','annt_dist','annt_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ignore_thre = 33\n",
    "annotators = list(df_dist[df_dist.annt_count >= ignore_thre]['annt_id'])\n",
    "kmeans_feature = np.array(list(df_dist[df_dist.annt_count >= ignore_thre]['annt_dist']))\n",
    "clusters = kmeans.fit_predict(kmeans_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pad_size = 10\n",
    "annt_clusters = [list(map(lambda x:x[1],filter(lambda x:x[0] == i, zip(clusters,annotators)))) for i in range(n_clusters)]\n",
    "namehead = \"test\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory already exist\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir('./models/classifier')\n",
    "except FileExistsError:\n",
    "    print('directory already exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/takayama/.local/lib/python3.5/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(return_sequences=False, recurrent_dropout=0.5, units=1024, batch_input_shape=(None, 10,...)`\n",
      "  \"\"\"\n",
      "/home/takayama/.local/lib/python3.5/site-packages/ipykernel_launcher.py:9: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(return_sequences=False, recurrent_dropout=0.5, units=1024, batch_input_shape=(None, 10,...)`\n",
      "  if __name__ == '__main__':\n",
      "/home/takayama/.local/lib/python3.5/site-packages/ipykernel_launcher.py:12: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  if sys.path[0] == '':\n",
      "/home/takayama/.local/lib/python3.5/site-packages/keras/models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12548 samples, validate on 1395 samples\n",
      "Epoch 1/30\n",
      "12544/12548 [============================>.] - ETA: 0s - loss: 0.1338Epoch 00000: val_loss improved from inf to 0.10399, saving model to ./models/classifier/test_k_3_*.weight\n",
      "12548/12548 [==============================] - 15s - loss: 0.1338 - val_loss: 0.1040\n",
      "Epoch 2/30\n",
      "12544/12548 [============================>.] - ETA: 0s - loss: 0.1066Epoch 00001: val_loss improved from 0.10399 to 0.09810, saving model to ./models/classifier/test_k_3_*.weight\n",
      "12548/12548 [==============================] - 15s - loss: 0.1066 - val_loss: 0.0981\n",
      "Epoch 3/30\n",
      "12544/12548 [============================>.] - ETA: 0s - loss: 0.0995Epoch 00002: val_loss did not improve\n",
      "12548/12548 [==============================] - 15s - loss: 0.0995 - val_loss: 0.0986\n",
      "Epoch 4/30\n",
      "12544/12548 [============================>.] - ETA: 0s - loss: 0.0968Epoch 00003: val_loss improved from 0.09810 to 0.09452, saving model to ./models/classifier/test_k_3_*.weight\n",
      "12548/12548 [==============================] - 15s - loss: 0.0969 - val_loss: 0.0945\n",
      "Epoch 5/30\n",
      "12544/12548 [============================>.] - ETA: 0s - loss: 0.0937Epoch 00004: val_loss did not improve\n",
      "12548/12548 [==============================] - 15s - loss: 0.0937 - val_loss: 0.0948\n",
      "Epoch 6/30\n",
      "12544/12548 [============================>.] - ETA: 0s - loss: 0.0908Epoch 00005: val_loss did not improve\n",
      "12548/12548 [==============================] - 15s - loss: 0.0908 - val_loss: 0.0970\n",
      "Epoch 7/30\n",
      "12544/12548 [============================>.] - ETA: 0s - loss: 0.0882Epoch 00006: val_loss improved from 0.09452 to 0.09433, saving model to ./models/classifier/test_k_3_*.weight\n",
      "12548/12548 [==============================] - 15s - loss: 0.0882 - val_loss: 0.0943\n",
      "Epoch 8/30\n",
      "12544/12548 [============================>.] - ETA: 0s - loss: 0.0845Epoch 00007: val_loss did not improve\n",
      "12548/12548 [==============================] - 15s - loss: 0.0844 - val_loss: 0.0963\n",
      "Epoch 9/30\n",
      "12544/12548 [============================>.] - ETA: 0s - loss: 0.0810Epoch 00008: val_loss did not improve\n",
      "12548/12548 [==============================] - 15s - loss: 0.0810 - val_loss: 0.0983\n",
      "Epoch 10/30\n",
      "12544/12548 [============================>.] - ETA: 0s - loss: 0.0777Epoch 00009: val_loss did not improve\n",
      "\n",
      "Epoch 00009: reducing learning rate to 0.0001500000071246177.\n",
      "12548/12548 [==============================] - 15s - loss: 0.0777 - val_loss: 0.1000\n",
      "Epoch 11/30\n",
      "12544/12548 [============================>.] - ETA: 0s - loss: 0.0685Epoch 00010: val_loss did not improve\n",
      "12548/12548 [==============================] - 15s - loss: 0.0685 - val_loss: 0.0999\n",
      "Epoch 12/30\n",
      "12544/12548 [============================>.] - ETA: 0s - loss: 0.0642Epoch 00011: val_loss did not improve\n",
      "12548/12548 [==============================] - 15s - loss: 0.0642 - val_loss: 0.1019\n",
      "Epoch 13/30\n",
      "12544/12548 [============================>.] - ETA: 0s - loss: 0.0615Epoch 00012: val_loss did not improve\n",
      "12548/12548 [==============================] - 15s - loss: 0.0615 - val_loss: 0.1047\n",
      "Epoch 14/30\n",
      "12544/12548 [============================>.] - ETA: 0s - loss: 0.0591Epoch 00013: val_loss did not improve\n",
      "\n",
      "Epoch 00013: reducing learning rate to 4.500000213738531e-05.\n",
      "12548/12548 [==============================] - 15s - loss: 0.0591 - val_loss: 0.1053\n",
      "Epoch 15/30\n",
      "12544/12548 [============================>.] - ETA: 0s - loss: 0.0536Epoch 00014: val_loss did not improve\n",
      "12548/12548 [==============================] - 15s - loss: 0.0536 - val_loss: 0.1064\n",
      "Epoch 00014: early stopping\n",
      "Train on 1683 samples, validate on 187 samples\n",
      "Epoch 1/30\n",
      "1664/1683 [============================>.] - ETA: 0s - loss: 0.1197Epoch 00000: val_loss improved from inf to 0.09317, saving model to ./models/classifier/test_k_3_*.weight\n",
      "1683/1683 [==============================] - 2s - loss: 0.1193 - val_loss: 0.0932\n",
      "Epoch 2/30\n",
      "1664/1683 [============================>.] - ETA: 0s - loss: 0.0889Epoch 00001: val_loss improved from 0.09317 to 0.08183, saving model to ./models/classifier/test_k_3_*.weight\n",
      "1683/1683 [==============================] - 2s - loss: 0.0890 - val_loss: 0.0818\n",
      "Epoch 3/30\n",
      "1664/1683 [============================>.] - ETA: 0s - loss: 0.0817Epoch 00002: val_loss improved from 0.08183 to 0.08059, saving model to ./models/classifier/test_k_3_*.weight\n",
      "1683/1683 [==============================] - 2s - loss: 0.0820 - val_loss: 0.0806\n",
      "Epoch 4/30\n",
      "1664/1683 [============================>.] - ETA: 0s - loss: 0.0779Epoch 00003: val_loss improved from 0.08059 to 0.07269, saving model to ./models/classifier/test_k_3_*.weight\n",
      "1683/1683 [==============================] - 2s - loss: 0.0775 - val_loss: 0.0727\n",
      "Epoch 5/30\n",
      "1664/1683 [============================>.] - ETA: 0s - loss: 0.0670Epoch 00004: val_loss did not improve\n",
      "1683/1683 [==============================] - 2s - loss: 0.0670 - val_loss: 0.0774\n",
      "Epoch 6/30\n",
      "1664/1683 [============================>.] - ETA: 0s - loss: 0.0682Epoch 00005: val_loss did not improve\n",
      "1683/1683 [==============================] - 2s - loss: 0.0682 - val_loss: 0.0808\n",
      "Epoch 7/30\n",
      "1664/1683 [============================>.] - ETA: 0s - loss: 0.0611Epoch 00006: val_loss improved from 0.07269 to 0.07192, saving model to ./models/classifier/test_k_3_*.weight\n",
      "1683/1683 [==============================] - 2s - loss: 0.0614 - val_loss: 0.0719\n",
      "Epoch 8/30\n",
      "1664/1683 [============================>.] - ETA: 0s - loss: 0.0582Epoch 00007: val_loss improved from 0.07192 to 0.06911, saving model to ./models/classifier/test_k_3_*.weight\n",
      "1683/1683 [==============================] - 2s - loss: 0.0584 - val_loss: 0.0691\n",
      "Epoch 9/30\n",
      "1664/1683 [============================>.] - ETA: 0s - loss: 0.0535Epoch 00008: val_loss did not improve\n",
      "1683/1683 [==============================] - 2s - loss: 0.0539 - val_loss: 0.0765\n",
      "Epoch 10/30\n",
      "1664/1683 [============================>.] - ETA: 0s - loss: 0.0555Epoch 00009: val_loss did not improve\n",
      "1683/1683 [==============================] - 2s - loss: 0.0554 - val_loss: 0.0753\n",
      "Epoch 11/30\n",
      "1664/1683 [============================>.] - ETA: 0s - loss: 0.0488Epoch 00010: val_loss did not improve\n",
      "\n",
      "Epoch 00010: reducing learning rate to 0.0001500000071246177.\n",
      "1683/1683 [==============================] - 2s - loss: 0.0489 - val_loss: 0.0775\n",
      "Epoch 12/30\n",
      "1664/1683 [============================>.] - ETA: 0s - loss: 0.0416Epoch 00011: val_loss did not improve\n",
      "1683/1683 [==============================] - 2s - loss: 0.0415 - val_loss: 0.0744\n",
      "Epoch 13/30\n",
      "1664/1683 [============================>.] - ETA: 0s - loss: 0.0372Epoch 00012: val_loss did not improve\n",
      "1683/1683 [==============================] - 2s - loss: 0.0372 - val_loss: 0.0765\n",
      "Epoch 14/30\n",
      "1664/1683 [============================>.] - ETA: 0s - loss: 0.0360Epoch 00013: val_loss did not improve\n",
      "1683/1683 [==============================] - 2s - loss: 0.0360 - val_loss: 0.0776\n",
      "Epoch 15/30\n",
      "1664/1683 [============================>.] - ETA: 0s - loss: 0.0346Epoch 00014: val_loss did not improve\n",
      "\n",
      "Epoch 00014: reducing learning rate to 4.500000213738531e-05.\n",
      "1683/1683 [==============================] - 2s - loss: 0.0345 - val_loss: 0.0783\n",
      "Epoch 16/30\n",
      "1664/1683 [============================>.] - ETA: 0s - loss: 0.0321Epoch 00015: val_loss did not improve\n",
      "1683/1683 [==============================] - 2s - loss: 0.0319 - val_loss: 0.0791\n",
      "Epoch 00015: early stopping\n",
      "Train on 4203 samples, validate on 467 samples\n",
      "Epoch 1/30\n",
      "4160/4203 [============================>.] - ETA: 0s - loss: 0.1707Epoch 00000: val_loss improved from inf to 0.13380, saving model to ./models/classifier/test_k_3_*.weight\n",
      "4203/4203 [==============================] - 5s - loss: 0.1704 - val_loss: 0.1338\n",
      "Epoch 2/30\n",
      "4160/4203 [============================>.] - ETA: 0s - loss: 0.1551Epoch 00001: val_loss did not improve\n",
      "4203/4203 [==============================] - 5s - loss: 0.1554 - val_loss: 0.1367\n",
      "Epoch 3/30\n",
      "4160/4203 [============================>.] - ETA: 0s - loss: 0.1448Epoch 00002: val_loss did not improve\n",
      "4203/4203 [==============================] - 5s - loss: 0.1447 - val_loss: 0.1391\n",
      "Epoch 4/30\n",
      "4160/4203 [============================>.] - ETA: 0s - loss: 0.1402Epoch 00003: val_loss did not improve\n",
      "\n",
      "Epoch 00003: reducing learning rate to 0.0001500000071246177.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4203/4203 [==============================] - 5s - loss: 0.1401 - val_loss: 0.1389\n",
      "Epoch 5/30\n",
      "4160/4203 [============================>.] - ETA: 0s - loss: 0.1288Epoch 00004: val_loss improved from 0.13380 to 0.12728, saving model to ./models/classifier/test_k_3_*.weight\n",
      "4203/4203 [==============================] - 5s - loss: 0.1289 - val_loss: 0.1273\n",
      "Epoch 6/30\n",
      "4160/4203 [============================>.] - ETA: 0s - loss: 0.1215Epoch 00005: val_loss improved from 0.12728 to 0.12519, saving model to ./models/classifier/test_k_3_*.weight\n",
      "4203/4203 [==============================] - 5s - loss: 0.1219 - val_loss: 0.1252\n",
      "Epoch 7/30\n",
      "4160/4203 [============================>.] - ETA: 0s - loss: 0.1169Epoch 00006: val_loss did not improve\n",
      "4203/4203 [==============================] - 5s - loss: 0.1169 - val_loss: 0.1259\n",
      "Epoch 8/30\n",
      "4160/4203 [============================>.] - ETA: 0s - loss: 0.1166Epoch 00007: val_loss did not improve\n",
      "4203/4203 [==============================] - 5s - loss: 0.1163 - val_loss: 0.1282\n",
      "Epoch 9/30\n",
      "4160/4203 [============================>.] - ETA: 0s - loss: 0.1116Epoch 00008: val_loss did not improve\n",
      "\n",
      "Epoch 00008: reducing learning rate to 4.500000213738531e-05.\n",
      "4203/4203 [==============================] - 5s - loss: 0.1116 - val_loss: 0.1286\n",
      "Epoch 10/30\n",
      "4160/4203 [============================>.] - ETA: 0s - loss: 0.1046Epoch 00009: val_loss did not improve\n",
      "4203/4203 [==============================] - 5s - loss: 0.1045 - val_loss: 0.1300\n",
      "Epoch 11/30\n",
      "4160/4203 [============================>.] - ETA: 0s - loss: 0.1025Epoch 00010: val_loss did not improve\n",
      "4203/4203 [==============================] - 5s - loss: 0.1027 - val_loss: 0.1284\n",
      "Epoch 12/30\n",
      "4160/4203 [============================>.] - ETA: 0s - loss: 0.1013Epoch 00011: val_loss did not improve\n",
      "4203/4203 [==============================] - 5s - loss: 0.1012 - val_loss: 0.1289\n",
      "Epoch 13/30\n",
      "4160/4203 [============================>.] - ETA: 0s - loss: 0.1005Epoch 00012: val_loss did not improve\n",
      "\n",
      "Epoch 00012: reducing learning rate to 1.3500000204658135e-05.\n",
      "4203/4203 [==============================] - 5s - loss: 0.1003 - val_loss: 0.1289\n",
      "Epoch 14/30\n",
      "4160/4203 [============================>.] - ETA: 0s - loss: 0.0973Epoch 00013: val_loss did not improve\n",
      "4203/4203 [==============================] - 5s - loss: 0.0973 - val_loss: 0.1284\n",
      "Epoch 00013: early stopping\n"
     ]
    }
   ],
   "source": [
    "for i,annt_ids in enumerate(annt_clusters):\n",
    "    target_annt = df_annotations[annt_ids][:-500].dropna(how='all')\n",
    "    indexes = list(target_annt.index)\n",
    "    vecs = df_vecs.iloc[indexes,:]\n",
    "    labels = np.array([np.mean(list(filter(lambda x:type(x) != str,annts)),axis=0) for annts in target_annt.fillna('nan').values])\n",
    "    user_x = sequence.pad_sequences(vecs['user'],pad_size,dtype=np.float32)\n",
    "    system_x = sequence.pad_sequences(vecs['system'],pad_size,dtype=np.float32)\n",
    "    tmp = list(zip(user_x,system_x,labels))\n",
    "    np.random.shuffle(tmp)\n",
    "    user_x,system_x,labels = map(np.array,zip(*tmp))\n",
    "    \n",
    "    decoder = Classifier(pad_size=pad_size).decoder\n",
    "\n",
    "    mc_cb = keras.callbacks.ModelCheckpoint(\n",
    "        './models/classifier/'+namehead+'_k_'+str(n_clusters)+'_*.weight', \n",
    "        monitor='val_loss', \n",
    "        verbose=1, \n",
    "        save_best_only=True,\n",
    "        save_weights_only=True, \n",
    "        mode='auto', \n",
    "        period=1\n",
    "    )\n",
    "    \n",
    "    es_cb = keras.callbacks.EarlyStopping(monitor='val_loss', patience=7, verbose=1, mode='auto')\n",
    "    rl_cb = keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', \n",
    "        factor=0.3, \n",
    "        patience=2, \n",
    "        verbose=1, \n",
    "        mode='auto', \n",
    "        epsilon=0.0001, \n",
    "        cooldown=2, \n",
    "        min_lr=0\n",
    "    )\n",
    "\n",
    "    history = decoder.fit(\n",
    "        [user_x, system_x], \n",
    "        labels,\n",
    "        batch_size=64, \n",
    "        nb_epoch=30,\n",
    "        callbacks=[mc_cb,rl_cb,es_cb],\n",
    "        validation_split = 0.1,\n",
    "        shuffle=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_annt = df_annotations[-500:].dropna(how='all')\n",
    "indexes = list(target_annt.index)\n",
    "vecs = df_vecs.iloc[indexes,:]\n",
    "valid_labels = np.array([np.mean(list(filter(lambda x:type(x) != str,annts)),axis=0) for annts in target_annt.fillna('nan').values])\n",
    "\n",
    "user_valid_x = sequence.pad_sequences(vecs['user'],pad_size,dtype=np.float32)\n",
    "system_valid_x = sequence.pad_sequences(vecs['system'],pad_size,dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/takayama/.local/lib/python3.5/site-packages/keras/engine/topology.py:1242: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  return cls(**config)\n"
     ]
    }
   ],
   "source": [
    "decoder = keras.models.model_from_yaml(open('./models/classifier/'+namehead+'_k_'+str(n_clusters)+'.yaml','r'))\n",
    "import glob\n",
    "res = []\n",
    "for modelpath in glob.glob('./models/classifier/'+namehead+'_k_'+str(n_clusters)+'_*.weight'):\n",
    "    decoder.load_weights(modelpath)\n",
    "    res.append(decoder.predict([user_valid_x,system_valid_x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fmeasure, recall, precision, accuracy, mse = utils.evaluate(utils.softmax(np.max(res,axis=0)),valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fmeasure             [ 0.6977492   0.24186047  0.28220859]\n",
      "recall               [ 0.57712766  0.40625     0.38333333]\n",
      "precision            [ 0.88211382  0.17218543  0.22330097]\n",
      "accuracy             0.532\n",
      "mse                  0.294089246591\n"
     ]
    }
   ],
   "source": [
    "print(\"Fmeasure\".ljust(20),fmeasure)\n",
    "print(\"recall\".ljust(20),recall)\n",
    "print(\"precision\".ljust(20),precision)\n",
    "print(\"accuracy\".ljust(20),accuracy)\n",
    "print(\"mse\".ljust(20),mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'get_shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-e6d2cb47c86f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/activations.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(x, axis)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIn\u001b[0m \u001b[0mcase\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \"\"\"\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mndim\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m     \"\"\"\n\u001b[0;32m--> 520\u001b[0;31m     \u001b[0mdims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'get_shape'"
     ]
    }
   ],
   "source": [
    "keras.activations.softmax(np.array([[1,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
