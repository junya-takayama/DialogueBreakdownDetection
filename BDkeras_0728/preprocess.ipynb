{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def corpusGenerator(fplist,train=False):\n",
    "    results = []\n",
    "    add = results.append\n",
    "    for fp in fplist:\n",
    "        data= json.load(open(fp))\n",
    "        logs = data['turns']\n",
    "        for i in range(1,len(logs),2):\n",
    "            if train:\n",
    "                annt = [a[\"breakdown\"] for a in logs[i+1][\"annotations\"]]\n",
    "                for an in annt:\n",
    "                    probDist = list(map(int,[an==\"O\",an==\"T\",an==\"X\"]))\n",
    "                    add((logs[i][\"utterance\"],logs[i+1][\"utterance\"],np.array(probDist)))\n",
    "            else:\n",
    "                annt = [a[\"breakdown\"] for a in logs[i+1][\"annotations\"]]\n",
    "                sm = len(annt)\n",
    "                probDist = (annt.count(\"O\")/sm,annt.count(\"T\")/sm,annt.count(\"X\")/sm)\n",
    "                add((logs[i][\"utterance\"],logs[i+1][\"utterance\"],np.array(probDist)))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mFilter(text,m):\n",
    "    tmp = [word for word in m.parse(text).strip().split(\"\\n\")[:-1]]\n",
    "    tmp1 = list(map(str.split,tmp))\n",
    "    tmp2 = list(filter(lambda x: \"名詞\" not in x[1],tmp1))\n",
    "    tmp3 = list(map(lambda x:x[0],tmp2))\n",
    "    return tmp3\n",
    "\n",
    "def tokenize(data,clustering=False):\n",
    "    if clustering:\n",
    "        m = MeCab.Tagger()\n",
    "        return [(mFilter(user,m),mFilter(system,m),probDist) for user,system,probDist in data]\n",
    "    else:\n",
    "        m = MeCab.Tagger(\"-Owakati\")\n",
    "        return [(m.parse(user).strip().split(),m.parse(system).strip().split(),probDist) for user,system,probDist in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def w2vtrain():\n",
    "    rawCorpus = \"\" #wordEmbedding学習用\n",
    "    for data in tokenized_trainData:\n",
    "        rawCorpus += \"\\n\" + \" \".join(data[0])\n",
    "        rawCorpus += \"\\n\" + \" \".join(data[1])\n",
    "    rawCorpus = rawCorpus.strip()\n",
    "    open(\"rawCorpus.txt\",\"w\").write(rawCorpus)\n",
    "    sentences = word2vec.LineSentence(\"rawCorpus.txt\")\n",
    "    w2v = word2vec.Word2Vec(sentences,window=1)\n",
    "    w2v.save(\"w2v.model\")\n",
    "    return w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(tokenizedData):\n",
    "    g = lambda x: list(filter(lambda word:word in w2v.wv.vocab.keys(), x))\n",
    "    f = lambda x: np.array([w2v.wv[word] for word in x])\n",
    "    return np.array([(f(g(user)),f(g(system)),probDist) for user,system,probDist in tokenizedData])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "#w2vpath='../BreakDialogue/word2vec.gensim.model'\n",
    "w2vpath='./w2v.model'\n",
    "try:\n",
    "    w2v = word2vec.Word2Vec.load(w2vpath)\n",
    "except:\n",
    "    w2v = w2vtrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "fp_rest = glob.glob('../projectnextnlp-chat-dialogue-corpus/json/rest1046/*.json')\n",
    "fp_rest.extend (glob.glob('./DCM/*.json'))\n",
    "fp_rest.extend(glob.glob('./DIT/*.json'))\n",
    "fp_rest.extend(glob.glob('./IRS/*.json'))\n",
    "fp_rest.extend(glob.glob('./dev/*.json'))\n",
    "fp_init = glob.glob('../projectnextnlp-chat-dialogue-corpus/json/init100/*.json')\n",
    "trainData = corpusGenerator(fp_rest,False)\n",
    "validData = corpusGenerator(fp_init)\n",
    "tokenized_trainData = tokenize(trainData)\n",
    "tokenized_validData = tokenize(validData)\n",
    "vectorized_trainData = vectorize(tokenized_trainData)\n",
    "vectorized_validData = vectorize(tokenized_validData)\n",
    "\n",
    "tmp = \"\"\"\n",
    "with open('corpus.pickle', mode='wb') as f:\n",
    "    pickle.dump({\n",
    "        \"raw\":{\"train\":trainData,\"valid\":validData},\n",
    "        \"tokenized\":{\"train\":tokenized_trainData,\"valid\":tokenized_validData},\n",
    "        \"vectorized\":{\"train\":vectorized_trainData,\"valid\":vectorized_validData},\n",
    "    },f)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M = reduce(lambda a,b: a if a> b else b,map(lambda x: max(len(x[0]),len(x[1])),tokenized_trainData))\n",
    "m = reduce(lambda a,b: a if a< b else b,map(lambda x: max(len(x[0]),len(x[1])),tokenized_trainData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
